# -*- coding: utf-8 -*-
"""weather_predictions_shit_done.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d8K4tXBq6Amq5C5wiulRYBQQ3PSjV18B
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install -q -U torch watermark

# %reload_ext watermark
# %watermark -v -p numpy,pandas,torch

import torch
import os
import numpy as np
import pandas as pd
from tqdm import tqdm
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from torch import nn, optim
import torch.nn.functional as F

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# %config InlineBackend.figure_format='retina'

sns.set(style='whitegrid', palette='muted', font_scale=1.2)
HAPPY_COLORS_PALETTE = ["#01BEFE", "#FFDD00", "#FF7D00", "#FF006D", "#93D30C", "#8F0240FF"]
sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))
rcParams['figure.figsize'] = 12, 8

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

!wget https://github.com/curiousily/Getting-Things-Done-with-Pytorch/files/7170332/weatherAUS.csv

df = pd.read_csv('weatherAUS.csv')

df.shape

df.head()

df.columns

df["RainTomorrow"].unique()

df.head()

df['RainToday'].replace({'No': 0, 'Yes': 1}, inplace = True)
df['RainTomorrow'].replace({'No': 0, 'Yes': 1}, inplace = True)

df = df.dropna(how="any")

df.shape

cols = df.columns

import matplotlib.pyplot as plt

# List of columns to use as X (exclude 'RainTomorrow')
x_cols = [col for col in cols if col != 'RainTomorrow']

# Iterate through each column in x_cols and plot against 'RainTomorrow'
for col in x_cols:
    plt.figure(figsize=(8, 6))  # Set figure size
    plt.scatter(x=df[col], y=df['RainTomorrow'], alpha=0.7)
    plt.title(f"{col} vs RainTomorrow")
    plt.xlabel(col)
    plt.ylabel("RainTomorrow")
    plt.grid(True)
    plt.show()

df.RainTomorrow.value_counts() / df.shape[0]

X = df[['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation',
       'Sunshine', 'WindGustSpeed',
       'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm',
       'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am',
       'Temp3pm', 'RainToday']]
y = df[['RainTomorrow']]

print(X.dtypes)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)

X_train = torch.from_numpy(X_train.to_numpy()).float()
y_train = torch.squeeze(torch.from_numpy(y_train.to_numpy()).float())

X_test = torch.from_numpy(X_test.to_numpy()).float()
y_test = torch.squeeze(torch.from_numpy(y_test.to_numpy()).float())

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self, n_features=17):  # Default to 17 features
        super(Net, self).__init__()
        self.fc1 = nn.Linear(n_features, 64)  # First hidden layer with more neurons
        self.bn1 = nn.BatchNorm1d(64)  # Batch normalization after the first layer
        self.fc2 = nn.Linear(64, 32)  # Second hidden layer
        self.bn2 = nn.BatchNorm1d(32)  # Batch normalization after the second layer
        self.fc3 = nn.Linear(32, 16)  # Third hidden layer
        self.fc4 = nn.Linear(16, 1)  # Output layer for binary classification
        self.dropout = nn.Dropout(0.3)  # Dropout for regularization

    def forward(self, x):
        x = F.relu(self.bn1(self.fc1(x)))  # Input -> FC1 -> BatchNorm -> ReLU
        x = F.relu(self.bn2(self.fc2(x)))  # FC2 -> BatchNorm -> ReLU
        x = self.dropout(x)  # Apply dropout
        x = F.relu(self.fc3(x))  # FC3 -> ReLU
        return torch.sigmoid(self.fc4(x))  # Sigmoid for binary classification

### use the 4 features here
print(X_train.shape[1])

net = Net(X_train.shape[1])

criterion = nn.BCELoss()

optimizer = optim.Adam(net.parameters(), lr=0.001)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

X_train = X_train.to(device)
y_train = y_train.to(device)

X_test = X_test.to(device)
y_test = y_test.to(device)

criterion.to(device)
net.to(device)

device

def calculate_accuracy(y_true, y_pred):
  predicted = y_pred.ge(.5).view(-1)
  return (y_true==predicted).sum().float() / len(y_true)

def round_tensor(t, decimal_places=3):
  return round(t.item(), decimal_places)

def display_grad():
  for name, param in net.named_parameters():
    print(f"Parameter Name: {name}")
    print(f"gradian: {param.grad}")

for epoch in range(10000):
  y_pred = net(X_train)

  y_pred = torch.squeeze(y_pred)
  train_loss = criterion(y_pred, y_train)
  if epoch %100==0:
    train_acc = calculate_accuracy(y_train, y_pred)

    y_test_pred = net(X_test)
    y_test_pred = torch.squeeze(y_test_pred)

    test_loss = criterion(y_test_pred, y_test)

    test_acc = calculate_accuracy(y_test, y_test_pred)
    print(f'''epoch {epoch}
    Train set - loss: {round_tensor(train_loss)}, accuracy: {round_tensor(train_acc)}
    Test set - loss: {round_tensor(test_loss)}, accuracy: {round_tensor(test_acc)}
    ''')

##    display_grad()

    optimizer.zero_grad()
##    display_grad()

    train_loss.backward()
##    display_grad()

    optimizer.step()
##    display_grad()

model_path = "model.pth"

torch.save(net, model_path)    Train set - loss: 0.364, accuracy: 0.851

net = torch.load(model_path)

classes = ["no rain", "raining"]

y_pred = net(X_test)

y_pred = y_pred.ge(.5).view(-1).cpu()
y_test = y_test.cpu()

print(classification_report(y_test, y_pred, target_names=classes))

cm = confusion_matrix(y_test, y_pred)
df_cm = pd.DataFrame(cm, index=classes, columns=classes)

hmap = sns.heatmap(df_cm, annot=True, fmt="d")
hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')
hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')
plt.ylabel('True label')
plt.xlabel('Predicted label')

